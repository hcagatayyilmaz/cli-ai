{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing an ASGI Server for LLM Requests\n",
    "\n",
    "There are two ways of implementing an ASGI server where we will make our requests to the LLM.\n",
    "\n",
    "## 1. Running the ASGI Server\n",
    "\n",
    "To test this, we first need to run `server.py`, which acts as our ASGI server. There are two main ways to run it:\n",
    "\n",
    "### A. Using `server.py`\n",
    "\n",
    "The `server.py` version gives more control over settings and is more customizable. Key features include:\n",
    "\n",
    "- GPU acceleration enabled\n",
    "- Proper error handling\n",
    "- Standard port and host settings\n",
    "- Configurable model parameters\n",
    "\n",
    "### B. Using the Command Line\n",
    "\n",
    "The built-in server from `llama-cpp-python` provides another way to run the ASGI server. This approach is:\n",
    "\n",
    "- More reliable\n",
    "- Better configured\n",
    "- Officially supported\n",
    "\n",
    "## 2. Running `main.py`\n",
    "\n",
    "Here, we can run `main.py`, which is the main file where we start sending requests from the dataset to be processed by the LLM with the required parameters. The results are saved into the `my_dataset` file.\n",
    "\n",
    "The command to run `main.py` is as follows:\n",
    "\n",
    "```bash\n",
    "poetry run python main.py --model arcee-agent --base_url http://localhost:8000/v1 --api_key \"dummy\" --dev\n",
    "```\n",
    "\n",
    "\n",
    "## Running the main.py with flags\n",
    "I've added `--train` and `--dev` flags to the main.py file. `--train` flag is used to process all examples without batches, and `--dev` flag is used to process first 100 examples with batch size 10. If no flag is provided, it will process all examples with batch size 1000.\n",
    "\n",
    "```bash\n",
    "\n",
    "# Training mode (no batches)\n",
    "poetry run python main.py --model arcee-agent --base_url http://localhost:8000/v1 --api_key \"dummy\" --train\n",
    "\n",
    "# Dev mode (100 examples, batch size 10)\n",
    "poetry run python main.py --model arcee-agent --base_url http://localhost:8000/v1 --api_key \"dummy\" --dev\n",
    "\n",
    "# Full mode (all examples, batch size 1000)\n",
    "poetry run python main.py --model arcee-agent --base_url http://localhost:8000/v1 --api_key \"dummy\"\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improvements\n",
    "\n",
    "I like working inside containers always since they are more reliable and easier to manage than my local machine. I have used containers for this type of challenge before, so it was very educative for me. Here, I created a Dockerfile and docker-compose. However, I was working on a MacBook with an M1/M2 chip, which is not fully supported in Docker. It was interesting to learn that GPU acceleration doesn't work in Docker containers on Apple Silicon (M1/M2) Macs because containers run in a Linux virtual machine, which can't access Apple's Metal framework needed for GPU acceleration.\n",
    "\n",
    "I also learned about adding an LLM model as a volume so that it loads faster and reduces the image size. Mounting the model as a volume allows the container to access it directly from the host system's storage without copying it into the container's filesystem. By keeping the model file outside the container and using a mount, you avoid inflating the container's size."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
